@article{Cer2018,
  abstract = {We present models for encoding sentences into embedding vectors that specifically target transfer learning to other NLP tasks. The models are efficient and result in accurate performance on diverse transfer tasks. Two variants of the encoding models allow for trade-offs between accuracy and compute resources. For both variants, we investigate and report the relationship between model complexity, resource consumption, the availability of transfer task training data, and task performance. Comparisons are made with baselines that use word level transfer learning via pretrained word embeddings as well as baselines do not use any transfer learning. We find that transfer learning using sentence embeddings tends to outperform word level transfer. With transfer learning via sentence embeddings, we observe surprisingly good performance with minimal amounts of supervised training data for a transfer task. We obtain encouraging results on Word Embedding Association Tests (WEAT) targeted at detecting model bias. Our pre-trained sentence encoding models are made freely available for download and on TF Hub.},
  author = {Daniel Cer and Yinfei Yang and Sheng-yi Kong and Nan Hua and Nicole Limtiaco and Rhomni St John and Noah Constant and Mario Guajardo-Céspedes and Steve Yuan and Chris Tar and Yun-Hsuan Sung and Brian Strope and Ray Kurzweil Google Research Mountain View},
  month = {3},
  title = {Universal Sentence Encoder},
  url = {https://arxiv.org/abs/1803.11175v2},
  year = {2018},
  keywords = {transformer, DAN}
}

@article{Devlin2018,
  abstract = {We introduce a new language representation model called BERT, which stands for Bidirectional Encoder Representations from Transformers. Unlike recent language representation models, BERT is designed to pre-train deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers. As a result, the pre-trained BERT model can be fine-tuned with just one additional output layer to create state-of-the-art models for a wide range of tasks, such as question answering and language inference, without substantial task-specific architecture modifications. BERT is conceptually simple and empirically powerful. It obtains new state-of-the-art results on eleven natural language processing tasks, including pushing the GLUE score to 80.5% (7.7% point absolute improvement), MultiNLI accuracy to 86.7% (4.6% absolute improvement), SQuAD v1.1 question answering Test F1 to 93.2 (1.5 point absolute improvement) and SQuAD v2.0 Test F1 to 83.1 (5.1 point absolute improvement).},
  author = {Jacob Devlin and Ming Wei Chang and Kenton Lee and Kristina Toutanova},
  isbn = {9781950737130},
  journal = {NAACL HLT 2019 - 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies - Proceedings of the Conference},
  month = {10},
  pages = {4171-4186},
  publisher = {Association for Computational Linguistics (ACL)},
  title = {BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding},
  url = {https://arxiv.org/abs/1810.04805v2},
  volume = {1},
  year = {2018},
  keywords = {transformer}
}

@article{Guan2011,
  abstract = {Based on an effective clustering algorithm-Affinity Propagation (AP)-we present in this paper a novel semisupervised text clustering algorithm, called Seeds Affinity Propagation (SAP). There are two main contributions in our approach: 1) a new similarity metric that captures the structural information of texts, and 2) a novel seed construction method to improve the semisupervised clustering process. To study the performance of the new algorithm, we applied it to the benchmark data set Reuters-21578 and compared it to two state-of-the-art clustering algorithms, namely, k-means algorithm and the original AP algorithm. Furthermore, we have analyzed the individual impact of the two proposed contributions. Results show that the proposed similarity metric is more effective in text clustering (F-measures ca. 21 percent higher than in the AP algorithm) and the proposed semisupervised strategy achieves both better clustering results and faster convergence (using only 76 percent iterations of the original AP). The complete SAP algorithm obtains higher F-measure (ca. 40 percent improvement over k-means and AP) and lower entropy (ca. 28 percent decrease over k-means and AP), improves significantly clustering execution time (20 times faster) in respect that k-means, and provides enhanced robustness compared with all other methods. © 2011 IEEE.},
  author = {Renchu Guan and Xiaohu Shi and Maurizio Marchese and Chen Yang and Yanchun Liang},
  doi = {10.1109/TKDE.2010.144},
  issn = {10414347},
  issue = {4},
  journal = {IEEE Transactions on Knowledge and Data Engineering},
  keywords = {Affinity propagation,cofeature set,significant cofeature set,text clustering,unilateral feature set},
  pages = {627-637},
  title = {Text clustering with Seeds Affinity Propagation},
  volume = {23},
  year = {2011}
}

@article{Lai2015,
  abstract = {Text classification is a foundational task in many NLP applications. Traditional text classifiers often rely on many human-designed features, such as dictionaries, knowledge bases and special tree kernels. In contrast to traditional methods, we introduce a recurrent convolutional neural network for text classification without human-designed features. In our model, we apply a recurrent structure to capture contextual information as far as possible when learning word representations, which may introduce considerably less noise compared to traditional window-based neural networks. We also employ a max-pooling layer that automatically judges which words play key roles in text classification to capture the key components in texts. We conduct experiments on four commonly used datasets. The experimental results show that the proposed method outperforms the state-of-the-art methods on several datasets, particularly on document-level datasets.},
  author = {Siwei Lai and Liheng Xu and Kang Liu and Jun Zhao},
  doi = {10.1609/AAAI.V29I1.9513},
  isbn = {9781577357018},
  issn = {2374-3468},
  issue = {1},
  journal = {Proceedings of the AAAI Conference on Artificial Intelligence},
  keywords = {text classification,word embedding, text clustering},
  month = {2},
  pages = {2267-2273},
  publisher = {AI Access Foundation},
  title = {Recurrent Convolutional Neural Networks for Text Classification},
  url = {https://ojs.aaai.org/index.php/AAAI/article/view/9513},
  volume = {29},
  year = {2015}
}

@article{Li2008,
  abstract = {Feature selection is an important method for improving the efficiency and accuracy of text categorization algorithms by removing redundant and irrelevant terms from the corpus. In this paper, we propose a new supervised feature selection method, named CHIR, which is based on the chi2 statistic and new statistical data that can measure the positive term-category dependency. We also propose a new text clustering algorithm, named text clustering with feature selection (TCFS). TCFS can incorporate CHIR to identify relevant features (i.e., terms) iteratively, and the clustering becomes a learning process. We compared TCFS and the K-means clustering algorithm in combination with different feature selection methods for various real data sets. Our experimental results show that TCFS with CHIR has better clustering accuracy in terms of the F-measure and the purity. © 2007 IEEE.},
  author = {Yanjun Li and Congnan Luo and Soon M. Chung},
  doi = {10.1109/TKDE.2007.190740},
  issn = {10414347},
  issue = {5},
  journal = {IEEE Transactions on Knowledge and Data Engineering},
  keywords = {Chi2 statistic,Feature selection,Performance analysis,Text clustering,Text mining},
  month = {5},
  pages = {641-652},
  title = {Text clustering with feature selection by using statistical data},
  volume = {20},
  year = {2008}
}

@article{Liu2019,
  abstract = {Language model pretraining has led to significant performance gains but careful comparison between different approaches is challenging. Training is computationally expensive, often done on private datasets of different sizes, and, as we will show, hyperparameter choices have significant impact on the final results. We present a replication study of BERT pretraining (Devlin et al., 2019) that carefully measures the impact of many key hyperparameters and training data size. We find that BERT was significantly undertrained, and can match or exceed the performance of every model published after it. Our best model achieves state-of-the-art results on GLUE, RACE and SQuAD. These results highlight the importance of previously overlooked design choices, and raise questions about the source of recently reported improvements. We release our models and code.},
  author = {Yinhan Liu and Myle Ott and Naman Goyal and Jingfei Du and Mandar Joshi and Danqi Chen and Omer Levy and Mike Lewis and Luke Zettlemoyer and Veselin Stoyanov and Paul G Allen},
  month = {7},
  title = {RoBERTa: A Robustly Optimized BERT Pretraining Approach},
  url = {https://arxiv.org/abs/1907.11692v1},
  year = {2019}
}

@article{Reimers2019,
  abstract = {BERT (Devlin et al., 2018) and RoBERTa (Liu et al., 2019) has set a new state-of-the-art performance on sentence-pair regression tasks like semantic textual similarity (STS). However, it requires that both sentences are fed into the network, which causes a massive computational overhead: Finding the most similar pair in a collection of 10,000 sentences requires about 50 million inference computations (~65 hours) with BERT. The construction of BERT makes it unsuitable for semantic similarity search as well as for unsupervised tasks like clustering. In this publication, we present Sentence-BERT (SBERT), a modification of the pretrained BERT network that use siamese and triplet network structures to derive semantically meaningful sentence embeddings that can be compared using cosine-similarity. This reduces the effort for finding the most similar pair from 65 hours with BERT / RoBERTa to about 5 seconds with SBERT, while maintaining the accuracy from BERT. We evaluate SBERT and SRoBERTa on common STS tasks and transfer learning tasks, where it outperforms other state-of-the-art sentence embeddings methods.},
  author = {Nils Reimers and Iryna Gurevych},
  doi = {10.18653/v1/d19-1410},
  isbn = {9781950737901},
  journal = {EMNLP-IJCNLP 2019 - 2019 Conference on Empirical Methods in Natural Language Processing and 9th International Joint Conference on Natural Language Processing, Proceedings of the Conference},
  month = {8},
  pages = {3982-3992},
  publisher = {Association for Computational Linguistics},
  title = {Sentence-BERT: Sentence Embeddings using Siamese BERT-Networks},
  url = {https://arxiv.org/abs/1908.10084v1},
  year = {2019},
  keywords = {text clustering, sentence embeddings}
}

@article{Vaswani2017,
  abstract = {The dominant sequence transduction models are based on complex recurrent or convolutional neural networks in an encoder-decoder configuration. The best performing models also connect the encoder and decoder through an attention mechanism. We propose a new simple network architecture, the Transformer, based solely on attention mechanisms, dispensing with recurrence and convolutions entirely. Experiments on two machine translation tasks show these models to be superior in quality while being more parallelizable and requiring significantly less time to train. Our model achieves 28.4 BLEU on the WMT 2014 English-to-German translation task, improving over the existing best results, including ensembles by over 2 BLEU. On the WMT 2014 English-to-French translation task, our model establishes a new single-model state-of-the-art BLEU score of 41.8 after training for 3.5 days on eight GPUs, a small fraction of the training costs of the best models from the literature. We show that the Transformer generalizes well to other tasks by applying it successfully to English constituency parsing both with large and limited training data.},
  author = {Ashish Vaswani and Noam Shazeer and Niki Parmar and Jakob Uszkoreit and Llion Jones and Aidan N. Gomez and Łukasz Kaiser and Illia Polosukhin},
  issn = {10495258},
  journal = {Advances in Neural Information Processing Systems},
  month = {6},
  pages = {5999-6009},
  publisher = {Neural information processing systems foundation},
  title = {Attention Is All You Need},
  url = {https://arxiv.org/abs/1706.03762v5},
  volume = {2017-December},
  year = {2017},
  keywords = {transformer}
}

@article{Wang2018,
  abstract = {Word embeddings are effective intermediate representations for capturing semantic regularities between words, when learning the representations of text sequences. We propose to view text classification as a label-word joint embedding problem: each label is embedded in the same space with the word vectors. We introduce an attention framework that measures the compatibility of embeddings between text sequences and labels. The attention is learned on a training set of labeled samples to ensure that, given a text sequence, the relevant words are weighted higher than the irrelevant ones. Our method maintains the interpretability of word embeddings, and enjoys a built-in ability to leverage alternative sources of information, in addition to input text sequences. Extensive results on the several large text datasets show that the proposed framework outperforms the state-of-the-art methods by a large margin, in terms of both accuracy and speed.},
  author = {Guoyin Wang and Chunyuan Li and Wenlin Wang and Yizhe Zhang and Dinghan Shen and Xinyuan Zhang and Ricardo Henao and Lawrence Carin},
  doi = {10.18653/v1/p18-1216},
  isbn = {9781948087322},
  journal = {ACL 2018 - 56th Annual Meeting of the Association for Computational Linguistics, Proceedings of the Conference (Long Papers)},
  month = {5},
  pages = {2321-2331},
  publisher = {Association for Computational Linguistics (ACL)},
  title = {Joint Embedding of Words and Labels for Text Classification},
  url = {https://arxiv.org/abs/1805.04174v1},
  volume = {1},
  year = {2018},
  keywords = {text clustering}
}

@article{Zhang2017,
  abstract = {Unsupervised clustering is one of the most fundamental challenges in machine learning. A popular hypothesis is that data are generated from a union of low-dimensional nonlinear manifolds; thus an approach to clustering is identifying and separating these manifolds. In this paper, we present a novel approach to solve this problem by using a mixture of autoencoders. Our model consists of two parts: 1) a collection of autoencoders where each autoencoder learns the underlying manifold of a group of similar objects, and 2) a mixture assignment neural network, which takes the concatenated latent vectors from the autoencoders as input and infers the distribution over clusters. By jointly optimizing the two parts, we simultaneously assign data to clusters and learn the underlying manifolds of each cluster.},
  author = {Dejiao Zhang and Ann Arbor and Mi Yifan Sun Technicolor Los Altos and Ca Brian Eriksson Adobe San Jose and Ca Laura Balzano},
  month = {12},
  title = {Deep Unsupervised Clustering Using Mixture of Autoencoders},
  url = {https://arxiv.org/abs/1712.07788v2},
  year = {2017},
  keywords = {text clustering}
}

